my_file = "/home/silvia/repos/predicting_fixation/1_datasets/simcomms/processed_data_simcomms_0.5_full_jun16"
maxdilfactor <- 0.01
mindilfactor <- 0.00025
my_family <- Gamma(link = "log")
# Load the input data
csv <- read.csv(my_file) %>%
select(-reached_fixation_at, -final_size, -initial_size, -transfer, -filt_shannon, -filt_even, -filename, -sample)
# OPTIONS -----------------------------------------------------------------
out_folder = "/home/silvia/repos/predicting_fixation/3_analysis/GLM/R/"
my_file = "/home/silvia/repos/predicting_fixation/1_datasets/simcomms/processed_data_simcomms_0.5_full_jun"
maxdilfactor <- 0.01
mindilfactor <- 0.00025
my_family <- Gamma(link = "log")
# Load the input data
csv <- read.csv(my_file) %>%
select(-reached_fixation_at, -final_size, -initial_size, -transfer, -filt_shannon, -filt_even, -filename, -sample)
csv["distrib"] <- ifelse(csv$distrib == "uniform", 1, 0)
csv["log(dilfactor)"] <- log(csv$dilfactor)
print(paste("Muestras procesadas:", nrow(csv)))
csv <- csv %>%
na.omit()
print(paste("Eliminando los NA:", nrow(csv)))
csv <- csv[csv$dilfactor < maxdilfactor & csv$dilfactor > mindilfactor, ]
print(paste("Filtrando por dilution factor (>", mindilfactor, "; <", maxdilfactor, "):", nrow(csv)))
flexplot(success~., data=csv)
flexplot(success~1, data=csv)
flexplot(sqrt(success)~1, data=csv)
csv$success2<-sqrt8
csv$success2<-sqrt(csv$success)
flexplot(sqrt(success2)~1, data=csv)
csv$success22<-sqrt(csv$success2)
flexplot(sqrt(success2)~1, data=csv)
flexplot(sqrt(success22)~1, data=csv)
flexplot(sqrt(success2)~1, data=csv, bins = 50)
flexplot(sqrt(success2)~1, data=csv, bins = 509)
flexplot(sqrt(success)~1, data=csv, bins = 509)
flexplot(sqrt(success22)~1, data=csv, bins = 509)
csv$success8<-(csv$success)**(1/8)
flexplot(sqrt(success8)~1, data=csv, bins = 509)
flexplot(sqrt(success8)~1, data=csv, bins = 50)
csv$success == 0
csv$success == 0 %>% sum
(csv$success == 0) %>% sum
csv$logsuccess8<-log((csv$success)**(1/8))
flexplot(logsuccess8~1, data=csv, bins = 50)
flexplot(logsuccess8~1, data=csv, bins = 500)
# libraries ---------------------------------------------------------------
library(party)
library(flexplot) #devtools::install_github("dustinfife/flexplot")
library(dplyr)
# visualization packages
library(ggplot2)
library(DHARMa)
library(sjPlot)
# OPTIONS -----------------------------------------------------------------
my_file = "/home/silvia/repos/predicting_fixation/1_datasets/simcomms/processed_data_simcomms_0.5_full_jun"
# family has to be logistic
my_family <- binomial(link='logit')
results <- list()
for (l in c(10, 25, 50, 100, 200, 400, 800, 1000)) {
limit <- l # if fixation takes more than <limit> cycles to happen, that's a failure too
out_folder = paste0("/home/silvia/repos/predicting_fixation/3_analysis/GLM/R/", l, "__")
# Load the input data
fcsv <- read.csv(my_file) %>%
select(-reached_fixation_at, -final_size, -initial_size, -transfer, -filt_shannon, -filt_even, -filename, -sample)
fcsv["distrib"] <- ifelse(fcsv$distrib == "uniform", 1, 0)
fcsv["log(dilfactor)"] <- log(fcsv$dilfactor)
print(paste("Muestras procesadas:", nrow(fcsv)))
# fcsv <- fcsv[fcsv$dilfactor < maxdilfactor & fcsv$dilfactor > mindilfactor, ]
# print(paste("Filtrando por dilution factor (>", mindilfactor, "; <", maxdilfactor, "):", nrow(fcsv)))
# Create "failure" variable
fcsv["failure"] <- is.na(fcsv["success"]) | fcsv["success"] > limit
### Change diversity metric again
failuremodel <- glm("failure ~ size:log(dilfactor) +  raw_even  + size  + log(dilfactor)", data = fcsv, family = my_family)
# assess model --------------------------------------------------------------
results[l]["summary"] <- summary(failuremodel)
results[l]["model"]   <- failuremodel
## first we check the residuals
# Deviance Residuals:
#   Min        1Q    Median        3Q       Max
# -2.70390  -0.13151  -0.00003  -0.00001   2.04691
## > 1Q and 3Q abs values should be similar.
## > median should be close to 0
## min and max should be under 3 (good)
## and also similar to each other (good)
## we can see how the levine test goes well here, stable variance between feature values
simulatedOutput <- simulateResiduals(failuremodel, integerResponse = T)
png(paste0(out_folder, "dharma_simulateResiduals_FAILURE.png"), width = 1000, height = 600)
plot(simulatedOutput)
dev.off()
tab_model(failuremodel)
# Get the z-values from the model summary
z_values <- summary(failuremodel)$coefficients[, "z value"]
png(paste0(out_folder, "z-values_FAILURE.png"), width = 1000, height = 600)
# Create a bar plot of the x-values
barplot(z_values,
names.arg = names(z_values),
xlab = "Variables",
ylab = "z-values",
main = "Z-values of Coefficients",
border = "black",
col = z_values
)
dev.off()
# Now we can run the anova() function on the model to analyze the table of deviance
anova(failuremodel, test="Chisq")
# Df Deviance Resid. Df Resid. Dev  Pr(>Chi)
# NULL                                 1844     6125.7
# raw_even             1      1.8      1843     6123.9 4.766e-07 ***
#   size                 1   4818.8      1842     1305.2 < 2.2e-16 ***
#   log(dilfactor)       1   1111.7      1841      193.5 < 2.2e-16 ***
#   size:log(dilfactor)  1     21.2      1840      172.3 < 2.2e-16 ***
# The difference between the null deviance and the residual deviance shows how our
# model is doing against the null model (a model with only the intercept). The
# wider this gap, the better.
# While no exact equivalent to the R2 of linear regression exists, the McFadden
# R2 index can be used to assess the model fit.
results[l]["mcfadden"] <- pscl::pR2(failuremodel)
# barplots ----------------------------------------------------------------
# feature importance (RF !!!!)
rf_estimates <- estimates(cforest(failure ~ size:log(dilfactor) +  raw_even  + size  + log(dilfactor),
controls = cforest_control(ntree = 400),
data=fcsv))
feature_importance <- rf_estimates$importance
results[l]["FI"] <- feature_importance
# bar plot for feature importance
feature_df <- data.frame(Feature = names(feature_importance),
Importance = feature_importance)
feature_plot <- ggplot(data = feature_df, aes(x = Feature, y = Importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(x = "Feature", y = "Importance") +
ggtitle("Feature Importance")
# display
png(paste0(out_folder, "failure_FI.png"), width = 600, height = 400)
print(feature_plot)
dev.off()
# coefs themselves (GLM)
coefficients <- coef(failuremodel)
# bar plot for coefficients
coef_df <- data.frame(Coefficient = names(coefficients),
Value = coefficients)
coef_plot <- ggplot(data = coef_df, aes(x = Coefficient, y = Value)) +
geom_bar(stat = "identity", fill = "lightpink") +
labs(x = "Coefficient", y = "Value") +
ggtitle("Coefficients") + coord_flip()
# display
png(paste0(out_folder, "failure_coefs.png"), width = 600, height = 400)
print(coef_plot)
dev.off()
}
results
l
# libraries ---------------------------------------------------------------
library(party)
library(flexplot) #devtools::install_github("dustinfife/flexplot")
library(dplyr)
# visualization packages
library(ggplot2)
library(DHARMa)
library(sjPlot)
# OPTIONS -----------------------------------------------------------------
my_file = "/home/silvia/repos/predicting_fixation/1_datasets/simcomms/processed_data_simcomms_0.5_full_jun"
# family has to be logistic
my_family <- binomial(link='logit')
results <- list()
# for (l in c(10, 25, 50, 100, 200, 400, 800, 1000)) {
for (l in c(100)) {
limit <- l # if fixation takes more than <limit> cycles to happen, that's a failure too
out_folder = paste0("/home/silvia/repos/predicting_fixation/3_analysis/GLM/R/", l, "__")
# Load the input data
fcsv <- read.csv(my_file) %>%
select(-reached_fixation_at, -final_size, -initial_size, -transfer, -filt_shannon, -filt_even, -filename, -sample)
fcsv["distrib"] <- ifelse(fcsv$distrib == "uniform", 1, 0)
fcsv["log(dilfactor)"] <- log(fcsv$dilfactor)
print(paste("Muestras procesadas:", nrow(fcsv)))
# fcsv <- fcsv[fcsv$dilfactor < maxdilfactor & fcsv$dilfactor > mindilfactor, ]
# print(paste("Filtrando por dilution factor (>", mindilfactor, "; <", maxdilfactor, "):", nrow(fcsv)))
# Create "failure" variable
fcsv["failure"] <- is.na(fcsv["success"]) | fcsv["success"] > limit
### Change diversity metric again
failuremodel <- glm("failure ~ size:log(dilfactor) +  raw_even  + size  + log(dilfactor)", data = fcsv, family = my_family)
# assess model --------------------------------------------------------------
results[[l]]["summary"] <- summary(failuremodel)
results[[l]]["model"]   <- failuremodel
## first we check the residuals
# Deviance Residuals:
#   Min        1Q    Median        3Q       Max
# -2.70390  -0.13151  -0.00003  -0.00001   2.04691
## > 1Q and 3Q abs values should be similar.
## > median should be close to 0
## min and max should be under 3 (good)
## and also similar to each other (good)
## we can see how the levine test goes well here, stable variance between feature values
simulatedOutput <- simulateResiduals(failuremodel, integerResponse = T)
png(paste0(out_folder, "dharma_simulateResiduals_FAILURE.png"), width = 1000, height = 600)
plot(simulatedOutput)
dev.off()
tab_model(failuremodel)
# Get the z-values from the model summary
z_values <- summary(failuremodel)$coefficients[, "z value"]
png(paste0(out_folder, "z-values_FAILURE.png"), width = 1000, height = 600)
# Create a bar plot of the x-values
barplot(z_values,
names.arg = names(z_values),
xlab = "Variables",
ylab = "z-values",
main = "Z-values of Coefficients",
border = "black",
col = z_values
)
dev.off()
# Now we can run the anova() function on the model to analyze the table of deviance
anova(failuremodel, test="Chisq")
# Df Deviance Resid. Df Resid. Dev  Pr(>Chi)
# NULL                                 1844     6125.7
# raw_even             1      1.8      1843     6123.9 4.766e-07 ***
#   size                 1   4818.8      1842     1305.2 < 2.2e-16 ***
#   log(dilfactor)       1   1111.7      1841      193.5 < 2.2e-16 ***
#   size:log(dilfactor)  1     21.2      1840      172.3 < 2.2e-16 ***
# The difference between the null deviance and the residual deviance shows how our
# model is doing against the null model (a model with only the intercept). The
# wider this gap, the better.
# While no exact equivalent to the R2 of linear regression exists, the McFadden
# R2 index can be used to assess the model fit.
results[[l]]["mcfadden"] <- pscl::pR2(failuremodel)
# barplots ----------------------------------------------------------------
# feature importance (RF !!!!)
rf_estimates <- estimates(cforest(failure ~ size:log(dilfactor) +  raw_even  + size  + log(dilfactor),
controls = cforest_control(ntree = 400),
data=fcsv))
feature_importance <- rf_estimates$importance
results[[l]]["FI"] <- feature_importance
# bar plot for feature importance
feature_df <- data.frame(Feature = names(feature_importance),
Importance = feature_importance)
feature_plot <- ggplot(data = feature_df, aes(x = Feature, y = Importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(x = "Feature", y = "Importance") +
ggtitle("Feature Importance")
# display
png(paste0(out_folder, "failure_FI.png"), width = 600, height = 400)
print(feature_plot)
dev.off()
# coefs themselves (GLM)
coefficients <- coef(failuremodel)
# bar plot for coefficients
coef_df <- data.frame(Coefficient = names(coefficients),
Value = coefficients)
coef_plot <- ggplot(data = coef_df, aes(x = Coefficient, y = Value)) +
geom_bar(stat = "identity", fill = "lightpink") +
labs(x = "Coefficient", y = "Value") +
ggtitle("Coefficients") + coord_flip()
# display
png(paste0(out_folder, "failure_coefs.png"), width = 600, height = 400)
print(coef_plot)
dev.off()
}
results
results[[l]]
# libraries ---------------------------------------------------------------
library(party)
library(flexplot) #devtools::install_github("dustinfife/flexplot")
library(dplyr)
# visualization packages
library(ggplot2)
library(DHARMa)
library(sjPlot)
# OPTIONS -----------------------------------------------------------------
my_file = "/home/silvia/repos/predicting_fixation/1_datasets/simcomms/processed_data_simcomms_0.5_full_jun"
# family has to be logistic
my_family <- binomial(link='logit')
results <- list()
# for (l in c(10, 25, 50, 100, 200, 400, 800, 1000)) {
for (l in c(100)) {
results[l] <- list()
limit <- l # if fixation takes more than <limit> cycles to happen, that's a failure too
out_folder = paste0("/home/silvia/repos/predicting_fixation/3_analysis/GLM/R/", l, "__")
# Load the input data
fcsv <- read.csv(my_file) %>%
select(-reached_fixation_at, -final_size, -initial_size, -transfer, -filt_shannon, -filt_even, -filename, -sample)
fcsv["distrib"] <- ifelse(fcsv$distrib == "uniform", 1, 0)
fcsv["log(dilfactor)"] <- log(fcsv$dilfactor)
print(paste("Muestras procesadas:", nrow(fcsv)))
# fcsv <- fcsv[fcsv$dilfactor < maxdilfactor & fcsv$dilfactor > mindilfactor, ]
# print(paste("Filtrando por dilution factor (>", mindilfactor, "; <", maxdilfactor, "):", nrow(fcsv)))
# Create "failure" variable
fcsv["failure"] <- is.na(fcsv["success"]) | fcsv["success"] > limit
### Change diversity metric again
failuremodel <- glm("failure ~ size:log(dilfactor) +  raw_even  + size  + log(dilfactor)", data = fcsv, family = my_family)
# assess model --------------------------------------------------------------
results[[l]]["summary"] <- summary(failuremodel)
results[[l]]["model"]   <- failuremodel
## first we check the residuals
# Deviance Residuals:
#   Min        1Q    Median        3Q       Max
# -2.70390  -0.13151  -0.00003  -0.00001   2.04691
## > 1Q and 3Q abs values should be similar.
## > median should be close to 0
## min and max should be under 3 (good)
## and also similar to each other (good)
## we can see how the levine test goes well here, stable variance between feature values
simulatedOutput <- simulateResiduals(failuremodel, integerResponse = T)
png(paste0(out_folder, "dharma_simulateResiduals_FAILURE.png"), width = 1000, height = 600)
plot(simulatedOutput)
dev.off()
tab_model(failuremodel)
# Get the z-values from the model summary
z_values <- summary(failuremodel)$coefficients[, "z value"]
png(paste0(out_folder, "z-values_FAILURE.png"), width = 1000, height = 600)
# Create a bar plot of the x-values
barplot(z_values,
names.arg = names(z_values),
xlab = "Variables",
ylab = "z-values",
main = "Z-values of Coefficients",
border = "black",
col = z_values
)
dev.off()
# Now we can run the anova() function on the model to analyze the table of deviance
anova(failuremodel, test="Chisq")
# Df Deviance Resid. Df Resid. Dev  Pr(>Chi)
# NULL                                 1844     6125.7
# raw_even             1      1.8      1843     6123.9 4.766e-07 ***
#   size                 1   4818.8      1842     1305.2 < 2.2e-16 ***
#   log(dilfactor)       1   1111.7      1841      193.5 < 2.2e-16 ***
#   size:log(dilfactor)  1     21.2      1840      172.3 < 2.2e-16 ***
# The difference between the null deviance and the residual deviance shows how our
# model is doing against the null model (a model with only the intercept). The
# wider this gap, the better.
# While no exact equivalent to the R2 of linear regression exists, the McFadden
# R2 index can be used to assess the model fit.
results[[l]]["mcfadden"] <- pscl::pR2(failuremodel)
# barplots ----------------------------------------------------------------
# feature importance (RF !!!!)
rf_estimates <- estimates(cforest(failure ~ size:log(dilfactor) +  raw_even  + size  + log(dilfactor),
controls = cforest_control(ntree = 400),
data=fcsv))
feature_importance <- rf_estimates$importance
results[[l]]["FI"] <- feature_importance
# bar plot for feature importance
feature_df <- data.frame(Feature = names(feature_importance),
Importance = feature_importance)
feature_plot <- ggplot(data = feature_df, aes(x = Feature, y = Importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(x = "Feature", y = "Importance") +
ggtitle("Feature Importance")
# display
png(paste0(out_folder, "failure_FI.png"), width = 600, height = 400)
print(feature_plot)
dev.off()
# coefs themselves (GLM)
coefficients <- coef(failuremodel)
# bar plot for coefficients
coef_df <- data.frame(Coefficient = names(coefficients),
Value = coefficients)
coef_plot <- ggplot(data = coef_df, aes(x = Coefficient, y = Value)) +
geom_bar(stat = "identity", fill = "lightpink") +
labs(x = "Coefficient", y = "Value") +
ggtitle("Coefficients") + coord_flip()
# display
png(paste0(out_folder, "failure_coefs.png"), width = 600, height = 400)
print(coef_plot)
dev.off()
}
results$l
results$l-l
results$l<-1
# libraries ---------------------------------------------------------------
library(party)
library(flexplot) #devtools::install_github("dustinfife/flexplot")
library(dplyr)
# visualization packages
library(ggplot2)
library(DHARMa)
library(sjPlot)
# OPTIONS -----------------------------------------------------------------
my_file = "/home/silvia/repos/predicting_fixation/1_datasets/simcomms/processed_data_simcomms_0.5_full_jun"
# family has to be logistic
my_family <- binomial(link='logit')
results <- list()
# for (l in c(10, 25, 50, 100, 200, 400, 800, 1000)) {
for (l in c(100)) {
limit <- l # if fixation takes more than <limit> cycles to happen, that's a failure too
out_folder = paste0("/home/silvia/repos/predicting_fixation/3_analysis/GLM/R/", l, "__")
# Load the input data
fcsv <- read.csv(my_file) %>%
select(-reached_fixation_at, -final_size, -initial_size, -transfer, -filt_shannon, -filt_even, -filename, -sample)
fcsv["distrib"] <- ifelse(fcsv$distrib == "uniform", 1, 0)
fcsv["log(dilfactor)"] <- log(fcsv$dilfactor)
print(paste("Muestras procesadas:", nrow(fcsv)))
# fcsv <- fcsv[fcsv$dilfactor < maxdilfactor & fcsv$dilfactor > mindilfactor, ]
# print(paste("Filtrando por dilution factor (>", mindilfactor, "; <", maxdilfactor, "):", nrow(fcsv)))
# Create "failure" variable
fcsv["failure"] <- is.na(fcsv["success"]) | fcsv["success"] > limit
### Change diversity metric again
failuremodel <- glm("failure ~ size:log(dilfactor) +  raw_even  + size  + log(dilfactor)", data = fcsv, family = my_family)
# assess model --------------------------------------------------------------
results$l["summary"] <- summary(failuremodel)
results$l["model"]   <- failuremodel
## first we check the residuals
# Deviance Residuals:
#   Min        1Q    Median        3Q       Max
# -2.70390  -0.13151  -0.00003  -0.00001   2.04691
## > 1Q and 3Q abs values should be similar.
## > median should be close to 0
## min and max should be under 3 (good)
## and also similar to each other (good)
## we can see how the levine test goes well here, stable variance between feature values
simulatedOutput <- simulateResiduals(failuremodel, integerResponse = T)
png(paste0(out_folder, "dharma_simulateResiduals_FAILURE.png"), width = 1000, height = 600)
plot(simulatedOutput)
dev.off()
tab_model(failuremodel)
# Get the z-values from the model summary
z_values <- summary(failuremodel)$coefficients[, "z value"]
png(paste0(out_folder, "z-values_FAILURE.png"), width = 1000, height = 600)
# Create a bar plot of the x-values
barplot(z_values,
names.arg = names(z_values),
xlab = "Variables",
ylab = "z-values",
main = "Z-values of Coefficients",
border = "black",
col = z_values
)
dev.off()
# Now we can run the anova() function on the model to analyze the table of deviance
anova(failuremodel, test="Chisq")
# Df Deviance Resid. Df Resid. Dev  Pr(>Chi)
# NULL                                 1844     6125.7
# raw_even             1      1.8      1843     6123.9 4.766e-07 ***
#   size                 1   4818.8      1842     1305.2 < 2.2e-16 ***
#   log(dilfactor)       1   1111.7      1841      193.5 < 2.2e-16 ***
#   size:log(dilfactor)  1     21.2      1840      172.3 < 2.2e-16 ***
# The difference between the null deviance and the residual deviance shows how our
# model is doing against the null model (a model with only the intercept). The
# wider this gap, the better.
# While no exact equivalent to the R2 of linear regression exists, the McFadden
# R2 index can be used to assess the model fit.
results$l["mcfadden"] <- pscl::pR2(failuremodel)
# barplots ----------------------------------------------------------------
# feature importance (RF !!!!)
rf_estimates <- estimates(cforest(failure ~ size:log(dilfactor) +  raw_even  + size  + log(dilfactor),
controls = cforest_control(ntree = 400),
data=fcsv))
feature_importance <- rf_estimates$importance
results$l["FI"] <- feature_importance
# bar plot for feature importance
feature_df <- data.frame(Feature = names(feature_importance),
Importance = feature_importance)
feature_plot <- ggplot(data = feature_df, aes(x = Feature, y = Importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(x = "Feature", y = "Importance") +
ggtitle("Feature Importance")
# display
png(paste0(out_folder, "failure_FI.png"), width = 600, height = 400)
print(feature_plot)
dev.off()
# coefs themselves (GLM)
coefficients <- coef(failuremodel)
# bar plot for coefficients
coef_df <- data.frame(Coefficient = names(coefficients),
Value = coefficients)
coef_plot <- ggplot(data = coef_df, aes(x = Coefficient, y = Value)) +
geom_bar(stat = "identity", fill = "lightpink") +
labs(x = "Coefficient", y = "Value") +
ggtitle("Coefficients") + coord_flip()
# display
png(paste0(out_folder, "failure_coefs.png"), width = 600, height = 400)
print(coef_plot)
dev.off()
}
warnings(... = )
warnings( )
results
failuremodel
results$l$model   <- failuremodel
results$l$model
feature_importance
c(114222,202347,50114,90839,75491,71643,61649,51113,58771,223811)
last = c(114222,202347,50114,90839,75491,71643,61649,51113,58771,223811)
sum(last)
last/sum(last)
?rlm
# libraries ---------------------------------------------------------------
library(party)
library(flexplot) #devtools::install_github("dustinfife/flexplot")
library(dplyr)
# visualization packages
library(ggplot2)
library(DHARMa)
library(sjPlot)
setwd("~/repos/predicting_fixation/3_analysis")
load("../figures/2__GLM_failure/GLM_failure.RData")
c(0.3333, 0.3333, 0.1112) %>% sun
c(0.3333, 0.3333, 0.1112) %>% sun
c(0.3333, 0.3333, 0.1112) %>% sum
c(0.3333, 0.3333, 0.3334) %>% sum
c(0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10) %>% sum()
c(0.3, 0.2, 0.15, 0.1, 0.08, 0.05, 0.04, 0.025, 0.015, 0.002) %>%  sum
1-(c(0.3, 0.2, 0.15, 0.1, 0.08, 0.05, 0.04, 0.025, 0.015, 0.002) %>%  sum)
c(0.3, 0.2, 0.15, 0.1, 0.08, 0.05, 0.04, 0.025, 0.015) %>% sum
c(0.3, 0.2, 0.15, 0.1, 0.08, 0.05, 0.04, 0.025, 0.015, 0.2) %>% sum
c(0.3, 0.2, 0.15, 0.1, 0.08, 0.05, 0.04, 0.025, 0.015, 0.02) %>% sum
c(0.3, 0.2, 0.15, 0.1, 0.08, 0.06, 0.05, 0.035, 0.015, 0.005) %>% sum
c(0.3, 0.2, 0.15, 0.1, 0.085, 0.06, 0.05, 0.035, 0.015, 0.005) %>% sum
sort(c(0.194, 0.048, 0.071, 0.056, 0.095, 0.052, 0.168, 0.135, 0.080, 0.102))
sort(c(0.194, 0.048, 0.071, 0.056, 0.095, 0.052, 0.168, 0.135, 0.080, 0.102)) %>% paste(sep= ", ")
sort(c(0.194, 0.048, 0.071, 0.056, 0.095, 0.052, 0.168, 0.135, 0.080, 0.102)) %>% paste(sep= ", ") %>%print
sort(c(0.194, 0.048, 0.071, 0.056, 0.095, 0.052, 0.168, 0.135, 0.080, 0.102), decreasing = T)
setwd("~/repos/predicting_fixation/1_datasets")
